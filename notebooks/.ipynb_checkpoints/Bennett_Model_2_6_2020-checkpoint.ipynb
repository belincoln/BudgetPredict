{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tells matplotlib to embed plots within the notebook\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belincoln\\repos\\BudgetPredict\\data\n"
     ]
    }
   ],
   "source": [
    "os.chdir('C:\\\\Users\\\\belincoln\\\\repos\\\\BudgetPredict')\n",
    "# Set working directory to the data folder so you can correctly read in the csv files\n",
    "%cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from DHS Contracts\n",
    "df = pd.read_csv('FY2019_070_Contracts_Full_20200110_1.csv', header = 0, usecols = ['contract_transaction_unique_key', \n",
    "                        'federal_action_obligation','total_dollars_obligated', 'base_and_exercised_options_value', \n",
    "                        'current_total_value_of_award', 'base_and_all_options_value','potential_total_value_of_award'],\n",
    "                 dtype = {'contract_transaction_unique_key':'str','federal_action_obligation': 'float',\n",
    "                        'total_dollars_obligated': 'float', 'base_and_exercised_options_value': 'float', \n",
    "                        'current_total_value_of_award': 'float', 'base_and_all_options_value': 'float',\n",
    "                        'potential_total_value_of_award': 'float'},\n",
    "                nrows = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 new features for analysis\n",
    "df['Percent awarded over potential total awarded'] = df['current_total_value_of_award'] / df['potential_total_value_of_award']\n",
    "df['Percent Cumulatively Obligated over potential total value of award'] = df['total_dollars_obligated'] / df['potential_total_value_of_award']\n",
    "df['Percent Cumulatively Obligated over total value already awarded'] = df['total_dollars_obligated'] / df['current_total_value_of_award']\n",
    "\n",
    "# Create Indicator Variable\n",
    "df['Indicator'] = df['federal_action_obligation']<-1000\n",
    "\n",
    "# set index to each transaction key\n",
    "df.set_index('contract_transaction_unique_key', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning of Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['federal_action_obligation', 'total_dollars_obligated',\n",
       "       'base_and_exercised_options_value', 'current_total_value_of_award',\n",
       "       'base_and_all_options_value', 'potential_total_value_of_award',\n",
       "       'Percent awarded over potential total awarded',\n",
       "       'Percent Cumulatively Obligated over potential total value of award',\n",
       "       'Percent Cumulatively Obligated over total value already awarded',\n",
       "       'Indicator'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 10)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('federal_action_obligation', axis =1, inplace = True)\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y =  df.iloc[:,:-1], df.loc[:,'Indicator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_dollars_obligated                                               1.369191e+06\n",
      "base_and_exercised_options_value                                      3.491436e+03\n",
      "current_total_value_of_award                                          5.933033e+05\n",
      "base_and_all_options_value                                            7.287967e+03\n",
      "potential_total_value_of_award                                        4.542930e+06\n",
      "Percent awarded over potential total awarded                          6.956322e-01\n",
      "Percent Cumulatively Obligated over potential total value of award    6.383087e-01\n",
      "Percent Cumulatively Obligated over total value already awarded       7.250327e-01\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def  featureNormalize(X):\n",
    "    \"\"\"\n",
    "    Normalizes the features in X. returns a normalized version of X where\n",
    "    the mean value of each feature is 0 and the standard deviation\n",
    "    is 1. This is often a good preprocessing step to do when working with\n",
    "    learning algorithms.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_norm : array_like\n",
    "        The normalized dataset of shape (m x n).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    First, for each feature dimension, compute the mean of the feature\n",
    "    and subtract it from the dataset, storing the mean value in mu. \n",
    "    Next, compute the  standard deviation of each feature and divide\n",
    "    each feature by it's standard deviation, storing the standard deviation \n",
    "    in sigma. \n",
    "    \n",
    "    Note that X is a matrix where each column is a feature and each row is\n",
    "    an example. You needto perform the normalization separately for each feature. \n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    You might find the 'np.mean' and 'np.std' functions useful.\n",
    "    \"\"\"\n",
    "    # You need to set these values correctly\n",
    "    X_norm = X.copy()\n",
    "    mu = np.zeros(X.shape[1])\n",
    "    sigma = np.zeros(X.shape[1])\n",
    "\n",
    "    # =========================== YOUR CODE HERE =====================\n",
    "    mu = np.mean(X,axis=0)\n",
    "    sigma = np.std(X,axis=0)\n",
    "    X_norm = (X-mu)/sigma\n",
    "\n",
    "    \n",
    "    # ================================================================\n",
    "    return X_norm, mu, sigma\n",
    "print(np.mean(X,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm, mu, sigma = featureNormalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "\n",
    "#### Sigmoid function\n",
    "\n",
    "the logistic regression hypothesis is defined as:\n",
    "\n",
    "$$ h_\\theta(x) = g(\\theta^T x)$$\n",
    "\n",
    "where function $g$ is the sigmoid function. The sigmoid function is defined as: \n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$.\n",
    "\n",
    " For large positive values of `x`, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. Evaluating `sigmoid(0)` should give you exactly 0.5. \n",
    " \n",
    " This is used for classification models (rather than regresssion models).\n",
    " \n",
    "<a id=\"sigmoid\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute sigmoid function given the input z.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        The input to the sigmoid function. This can be a 1-D vector \n",
    "        or a 2-D matrix. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    g : array_like\n",
    "        The computed sigmoid function. g has the same shape as z, since\n",
    "        the sigmoid is computed element-wise on z.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the sigmoid of each value of z (z can be a matrix, vector or scalar).\n",
    "    \"\"\"\n",
    "    # convert input to a numpy array\n",
    "    z = np.array(z)\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    temp = 1 + np.power(np.e,-z)\n",
    "    g = 1 / temp\n",
    "    \n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the data matrix appropriately, and add ones for the intercept term\n",
    "m, n = X.shape\n",
    "# Add intercept term to X\n",
    "X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a function `costFunction` to return the cost and gradient. Recall that the cost function in logistic regression is\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ -y^{(i)} \\log\\left(h_\\theta\\left( x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - h_\\theta\\left( x^{(i)} \\right) \\right) \\right]$$\n",
    "\n",
    "and the gradient (derivative) of the cost is a vector of the same length as $\\theta$ where the $j^{th}$\n",
    "element (for $j = 0, 1, \\cdots , n$) is defined as follows:\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} $$\n",
    "\n",
    "Note that while this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $h_\\theta(x)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(theta, X, y):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "        The parameters for logistic regression. This a vector\n",
    "        of shape (n+1, ).\n",
    "    \n",
    "    X : array_like\n",
    "        The input dataset of shape (m x n+1) where m is the total number\n",
    "        of data points and n is the number of features. We assume the \n",
    "        intercept has already been added to the input.\n",
    "    \n",
    "    y : arra_like\n",
    "        Labels for the input. This is a vector of shape (m, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the cost function. \n",
    "    \n",
    "    grad : array_like\n",
    "        A vector of shape (n+1, ) which is the gradient of the cost\n",
    "        function with respect to theta, at the current values of theta.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the cost of a particular choice of theta. You should set J to \n",
    "    the cost. Compute the partial derivatives and set grad to the partial\n",
    "    derivatives of the cost w.r.t. each parameter in theta.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    h = sigmoid(np.dot(X,theta))\n",
    "    temp = -y*np.log(h) - (1-y)*np.log(1-h)\n",
    "    J = (1/m)*np.sum(temp)\n",
    "    grad = (1/m)*np.dot(X.T,(h-y))\n",
    "    # =============================================================\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0.6931471805599452\n",
      "Cost at initial theta (zeros):0.6931471805599452\n",
      "Gradient at initial theta (zeros):\n",
      "[ 0.395       0.00041043  0.0894244  -0.02363788  0.08861904  0.00340422\n",
      " -0.04866998 -0.06038187 -0.04256158]\n",
      "Cost at test theta: 2.167\n",
      "Gradient at test theta: [-0.07397369  0.03020513  0.12720242  0.09048648  0.15037709  0.08117861\n",
      " -0.04605166 -0.05365705 -0.03451056]\n"
     ]
    }
   ],
   "source": [
    "# Initialize fitting parameters\n",
    "initial_theta = np.zeros(n+1)\n",
    "print(initial_theta)\n",
    "cost, grad = costFunction(initial_theta, X, y)\n",
    "print(cost)\n",
    "\n",
    "print('Cost at initial theta (zeros):' + str(cost))\n",
    "\n",
    "\n",
    "print('Gradient at initial theta (zeros):')\n",
    "print(grad)\n",
    "\n",
    "\n",
    "# Compute and display cost and gradient with non-zero theta\n",
    "test_theta = np.array([-24, 0.2, 0.2,3,4,5,6,7,8])\n",
    "cost, grad = costFunction(test_theta, X, y)\n",
    "\n",
    "print('Cost at test theta: {:.3f}'.format(cost))\n",
    "\n",
    "\n",
    "print('Gradient at test theta: ' + str(grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at theta found by optimize.minimize: 0.256\n",
      "Expected cost (approx): 0.203\n",
      "\n",
      "theta:\n",
      "[-5.28533545 -0.21485466 -2.27427476  0.41448222 -2.37878087 -0.07412975\n",
      "  0.8869706   1.33013767  0.32693015]\n",
      "Expected theta (approx):\n",
      "\t[-25.161, 0.206, 0.201]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belincoln\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\belincoln\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "# set options for optimize.minimize\n",
    "options= {'maxiter': 400}\n",
    "\n",
    "# see documention for scipy's optimize.minimize  for description about\n",
    "# the different parameters\n",
    "# The function returns an object `OptimizeResult`\n",
    "# We use truncated Newton algorithm for optimization which is \n",
    "# equivalent to MATLAB's fminunc\n",
    "# See https://stackoverflow.com/questions/18801002/fminunc-alternate-in-numpy\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_theta,\n",
    "                        (X, y),\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# the fun property of `OptimizeResult` object returns\n",
    "# the value of costFunction at optimized theta\n",
    "cost = res.fun\n",
    "\n",
    "# the optimized theta is in the x property\n",
    "theta = res.x\n",
    "\n",
    "# Print theta to screen\n",
    "print('Cost at theta found by optimize.minimize: {:.3f}'.format(cost))\n",
    "print('Expected cost (approx): 0.203\\n');\n",
    "\n",
    "print('theta:')\n",
    "print(theta)\n",
    "print('Expected theta (approx):\\n\\t[-25.161, 0.206, 0.201]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfeatures = len(df.columns) - 2\\nfor i in range(features):\\n    fig = pyplot.figure()\\n    mask  = y == 1\\n    pyplot.plot(X[mask].iloc[:,i], X[mask].iloc[:, i+1], 'k*', lw=2, ms=10)\\n    pyplot.plot(X[~mask].iloc[:, i], X[~mask].iloc[:, i+1], 'ko', mfc='y', ms=8, mec='k', mew=1)\\n   \\n    # add axes labels\\n    pyplot.xlabel(df.columns[i])\\n    pyplot.ylabel(df.columns[i+1])\\n    pyplot.legend(['Sweep', 'Not a Sweep'])\\n    pass\\n\""
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This produces a scatter plot with each of the features, \n",
    "# but does not cover each permutation of features\n",
    "# 10 min runtime, I should rethink this\n",
    "\n",
    "'''\n",
    "features = len(df.columns) - 2\n",
    "for i in range(features):\n",
    "    fig = pyplot.figure()\n",
    "    mask  = y == 1\n",
    "    pyplot.plot(X[mask].iloc[:,i], X[mask].iloc[:, i+1], 'k*', lw=2, ms=10)\n",
    "    pyplot.plot(X[~mask].iloc[:, i], X[~mask].iloc[:, i+1], 'ko', mfc='y', ms=8, mec='k', mew=1)\n",
    "   \n",
    "    # add axes labels\n",
    "    pyplot.xlabel(df.columns[i])\n",
    "    pyplot.ylabel(df.columns[i+1])\n",
    "    pyplot.legend(['Sweep', 'Not a Sweep'])\n",
    "    pass\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
